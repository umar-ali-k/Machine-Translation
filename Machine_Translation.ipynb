{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umar-ali-k/Machine-Translation/blob/master/Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1Jh2BAEf2DI",
        "colab_type": "text"
      },
      "source": [
        "## Translate French sentences into English sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUdN8H_1fCD3",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBJ_41cYZZ6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "import numpy as np\n",
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eydsc1VffHx0",
        "colab_type": "text"
      },
      "source": [
        "## Install tensorflow 1.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7zuW9bZacpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==1.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKvfz38fO-K",
        "colab_type": "text"
      },
      "source": [
        "# Import all the necessary things which would be required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rqXvUXVZbK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tf.keras.models import Model  \n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdH6ctjQfctk",
        "colab_type": "text"
      },
      "source": [
        "## Get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAOZ54lBZq4V",
        "colab_type": "code",
        "outputId": "50d74351-f8bc-45b2-dbca-594c30d1af6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "import os\n",
        "work_dir = \"/content/TensorFlow-Tutorials/\"\n",
        "if os.getcwd() != work_dir:\n",
        "  !git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
        "os.chdir(work_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Tutorials'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 480 (delta 2), reused 5 (delta 2), pack-reused 471\u001b[K\n",
            "Receiving objects: 100% (480/480), 62.52 MiB | 31.49 MiB/s, done.\n",
            "Resolving deltas: 100% (223/223), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOwpVZe2fh2C",
        "colab_type": "text"
      },
      "source": [
        "Import europarl.py file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U4BQf9kbrFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import europarl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI5fmFPHfmN4",
        "colab_type": "text"
      },
      "source": [
        "## Language Code is 'fr' which means we are going to translate french sentences into english sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDj9bISHb75y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "language_code='fr'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6QuZ7ZMcFE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hwnuHagIVZ",
        "colab_type": "text"
      },
      "source": [
        "## This will automatically download and extract the data-files if you don't have them already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl8agnelcHrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5ab9ac01-4910-488d-f798-d2d75e8c6bab"
      },
      "source": [
        "europarl.maybe_download_and_extract(language_code=language_code)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1DcvRTNgSno",
        "colab_type": "text"
      },
      "source": [
        "## Load the texts for the source-language, here we use French."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt0H3w_8gYBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_src = europarl.load_data(english=False,\n",
        "                              language_code=language_code)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPs67xDgfCR",
        "colab_type": "text"
      },
      "source": [
        "## Load the texts for the destination-language, here we use English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mA4gYCDggP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_dest = europarl.load_data(english=True,\n",
        "                               language_code=language_code,\n",
        "                               start=mark_start,\n",
        "                               end=mark_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jYrZahZgwby",
        "colab_type": "text"
      },
      "source": [
        "# We will build a model to translate from the source language (French) to the destination language (English). If you want to make the inverse translation you can merely exchange the source and destination data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do9pIicwg8u0",
        "colab_type": "text"
      },
      "source": [
        "## Example Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YViVpG9xhIfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06eafefd-89de-4906-a792-adc08ad38c0f"
      },
      "source": [
        "idx = 2\n",
        "data_src[idx]\n",
        "#This will be our french sentence"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772MYTUChQvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4fa39a14-8350-4eee-8ebe-600d5d37ae41"
      },
      "source": [
        "data_dest[idx]\n",
        "#This will be our english sentence"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzX6ixyhlty",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizer\n",
        "Neural Networks cannot work directly on text-data. We use a two-step process to convert text into numbers that can be used in a neural network. The first step is to convert text-words into so-called integer-tokens. The second step is to convert integer-tokens into vectors of floating-point numbers using a so-called embedding-layer. See Tutorial #20 for a more detailed explanation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ4M5iNkhr2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set the maximum number of words in our vocabulary. This means that we will only use e.g. the 10000 most frequent words in the data-set. We use the same number for both the source and destination languages, but these could be different.\n",
        "num_words = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NRswAjyhw5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "class TokenizerWrap(Tokenizer):\n",
        "    \n",
        "    \n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "      \n",
        "       \n",
        "\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "\n",
        "        # Create the vocabulary from the texts.\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "\n",
        "        # Convert all texts to lists of integer-tokens.\n",
        "        # Note that the sequences may have different lengths.\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "            # Reverse the token-sequences.\n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "        \n",
        "            # Sequences that are too long should now be truncated\n",
        "            # at the beginning, which corresponds to the end of\n",
        "            # the original sequences.\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            # Sequences that are too long should be truncated\n",
        "            # at the end.\n",
        "            truncating = 'post'\n",
        "\n",
        "        # The number of integer-tokens in each sequence.\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "\n",
        "        # Max number of tokens to use in all sequences.\n",
        "        # We will pad / truncate all sequences to this length.\n",
        "        # This is a compromise so we save a lot of memory and\n",
        "        # only have to truncate maybe 5% of all the sequences.\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "\n",
        "        # Pad / truncate all token-sequences to the given length.\n",
        "        # This creates a 2-dim numpy matrix that is easier to use.\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
        "\n",
        "        # Create a list of the individual words.\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        "        # Concatenate the words to a single string\n",
        "        # with space between all the words.\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
        "       \n",
        "\n",
        "        # Convert to tokens. Note that we assume there is only\n",
        "        # a single text-string so we wrap it in a list.\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            # Reverse the tokens.\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "\n",
        "            \n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            # Sequences that are too long should be truncated\n",
        "            # at the end.\n",
        "            truncating = 'post'\n",
        "\n",
        "        if padding:\n",
        "            # Pad and truncate sequences to the given length.\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding='pre',\n",
        "                                   truncating=truncating)\n",
        "\n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvXXzu-uiOta",
        "colab_type": "text"
      },
      "source": [
        "Now create a tokenizer for the source-language. Note that we pad zeros at the beginning ('pre') of the sequences. We also reverse the sequences of tokens because the research literature suggests that this might improve performance, because the last words seen by the encoder match the first words produced by the decoder, so short-term dependencies are supposedly modelled more accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQe4Ve_EiQR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bda53eec-c812-42b8-86ba-c99c225d7e46"
      },
      "source": [
        "%%time\n",
        "tokenizer_src = TokenizerWrap(texts=data_src,\n",
        "                              padding='pre',\n",
        "                              reverse=True,\n",
        "                              num_words=num_words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 3s, sys: 1.52 s, total: 2min 5s\n",
            "Wall time: 2min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Jtv5mciTgY",
        "colab_type": "text"
      },
      "source": [
        "Now create the tokenizer for the destination language. We need a tokenizer for both the source- and destination-languages because their vocabularies are different. Note that this tokenizer does not reverse the sequences and it pads zeros at the end ('post') of the arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwEaeOfPiS69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1679abcc-cd60-421c-a7be-c1d12ed78e56"
      },
      "source": [
        "%%time\n",
        "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 23s, sys: 1.61 s, total: 1min 25s\n",
            "Wall time: 1min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABXI0z5pjq5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5c99bdf5-4cc1-4f47-8916-52b38cf1abfc"
      },
      "source": [
        "tokens_src = tokenizer_src.tokens_padded\n",
        "tokens_dest = tokenizer_dest.tokens_padded\n",
        "print(tokens_src.shape)\n",
        "print(tokens_dest.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2007723, 55)\n",
            "(2007723, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKNCBQU3jwo6",
        "colab_type": "text"
      },
      "source": [
        "This is the integer-token used to mark the beginning of a text in the destination-language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoFfA5Nmjz1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "200c61bc-f535-4f71-a25e-b96df5c814c2"
      },
      "source": [
        "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
        "token_start"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXeauwJlj9MY",
        "colab_type": "text"
      },
      "source": [
        "This is the integer-token used to mark the end of a text in the destination-language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQAP2CFqj-mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cdbed56-11c6-4fb3-fbef-c63f2d543d95"
      },
      "source": [
        "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
        "token_end"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbK0oYsAkYMD",
        "colab_type": "text"
      },
      "source": [
        "## Training Data\n",
        "Now that the data-set has been converted to sequences of integer-tokens that are padded and truncated and saved in numpy arrays, we can easily prepare the data for use in training the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg9Jr1fBkZrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = tokens_src\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTCEi1mQk0ee",
        "colab_type": "text"
      },
      "source": [
        "The input and output data for the decoder is identical, except shifted one time-step. We can use the same numpy array to save memory by slicing it, which merely creates different 'views' of the same data in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCUrGqn-kzwc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e8af1f8-bf9a-4350-a941-8b657953ca15"
      },
      "source": [
        "decoder_input_data = tokens_dest[:, :-1]\n",
        "decoder_input_data.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2007723, 55)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVZfXUx3lFek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "714e52b1-c88d-4869-aff0-f216c564ef8f"
      },
      "source": [
        "decoder_output_data = tokens_dest[:, 1:]\n",
        "decoder_output_data.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2007723, 55)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLqvIypClblm",
        "colab_type": "text"
      },
      "source": [
        "# Create the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDtm8E00lgOk",
        "colab_type": "text"
      },
      "source": [
        "# Create the Encoder\n",
        "First we create the encoder-part of the neural network which maps a sequence of integer-tokens to a \"thought vector\". We will use the so-called functional API of Keras for this, where we first create the objects for all the layers of the neural network and then we connect them later, this allows for more flexibility than the so-called sequential API in Keras, which is useful when experimenting with more complicated architectures and ways of connecting the encoder and decoder.\n",
        "\n",
        "This is the input for the encoder which takes batches of integer-token sequences. The None indicates that the sequences can have arbitrary length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhXnj-8IliiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = Input(shape=(None, ), name='encoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVlacqKUlph5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 128\n",
        "\n",
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')\n",
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWuR_6aHl0Ui",
        "colab_type": "text"
      },
      "source": [
        "This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anpw2IOAl2ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJvYpuatl41t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This helper-function connects all the layers of the encoder\n",
        "\n",
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "\n",
        "    # Connect all the GRU-layers.\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output = net\n",
        "    \n",
        "    return encoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-fz8dGAmC0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2b6e1c61-4c31-4f8a-8ed9-fdb12f8384c0"
      },
      "source": [
        "encoder_output = connect_encoder()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9HaIZdOmGbG",
        "colab_type": "text"
      },
      "source": [
        "## Create the Decoder\n",
        "Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens.\n",
        "\n",
        "The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHRObTTmF_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJFiLhkxmVCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = Input(shape=(None, ), name='decoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op8bEyswmf0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qPTbuoPmjtS",
        "colab_type": "text"
      },
      "source": [
        "This creates the 3 GRU layers of the decoder. Note that they all return sequences because we ultimately want to output a sequence of integer-tokens that can be converted into a text-sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTsT6wBUmhpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePR8QrZhmwgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_dense = Dense(num_words,\n",
        "                      activation='linear',\n",
        "                      name='decoder_output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piEz5Hr4m1AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def connect_decoder(initial_state):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "\n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU-layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhtf0hVEnAC-",
        "colab_type": "text"
      },
      "source": [
        "## Connect and Create the Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-2iwvI4nA8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output = connect_decoder(initial_state=encoder_output)\n",
        "\n",
        "model_train = Model(inputs=[encoder_input, decoder_input],\n",
        "                    outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3whWuv3nXdu",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8YnICRnO1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "  \n",
        "\n",
        "    \n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "\n",
        "  \n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Qbenk6ni1Z",
        "colab_type": "text"
      },
      "source": [
        "## Compile the Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnmjZr7nnfRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsI56EyEnoAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Rm1WR9nqrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "48cca1c5-c626-4fa7-9b77-fc24c0452880"
      },
      "source": [
        "model_train.compile(optimizer=optimizer,\n",
        "                    loss=sparse_cross_entropy,\n",
        "                    target_tensors=[decoder_target])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcemfS-Tolmp",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFxL3t42omdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = \\\n",
        "{\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACheu3d5oo3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_data = \\\n",
        "{\n",
        "    'decoder_output': decoder_output_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4fVQNeworTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "128a3f73-570b-4149-99d1-91fa9d5cf935"
      },
      "source": [
        "#We want a validation-set of 10000 sequences but Keras needs this number as a fraction.\n",
        "validation_split = 10000 / len(encoder_input_data)\n",
        "validation_split"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004980766769121039"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnYTq2PdqNwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=512,\n",
        "                epochs=7,\n",
        "                validation_split=validation_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avarVIILpjlb",
        "colab_type": "text"
      },
      "source": [
        "# Translate Texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVIQYXnco8S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "    \n",
        "\n",
        "  \n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    .\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_dest.max_tokens\n",
        "\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "   \n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        \n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "       \n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    \n",
        "    # Print the input-text.\n",
        "    print(\"Input text:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "    # Print the translated output-text.\n",
        "    print(\"Translated text:\")\n",
        "    print(output_text)\n",
        "    print()\n",
        "\n",
        "    # Optionally print the true translated text.\n",
        "    if true_output_text is not None:\n",
        "        print(\"True output text:\")\n",
        "        print(true_output_text)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}